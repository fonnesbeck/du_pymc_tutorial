---
title: "Getting Started with PyMC v5"
subtitle: "Data Umbrella Tutorial"
author: "Christopher Fonnesbeck<br><span style='font-size: 0.8em; color: #888;'>PyMC Labs</span>"
format:
  revealjs:
    theme: dark
    slide-number: true
    chalkboard: true
    preview-links: true
    transition: slide
    background-transition: fade
    incremental: false
    code-line-numbers: true
    highlight-style: github-dark
    width: 1600
    height: 900
    margin: 0.1
    min-scale: 0.2
    max-scale: 2.0
    css: |
      .reveal h1, .reveal h2, .reveal h3 {
        line-height: 1.2;
        margin-bottom: 1em;
      }
      .reveal .slides section {
        text-align: left;
      }
      .reveal .slides section[data-state="title-slide"] {
        text-align: center;
      }
      .reveal code {
        font-size: 0.85em;
        line-height: 1.3;
      }
      .reveal pre code {
        font-size: 0.75em;
        line-height: 1.4;
        max-height: 500px;
        overflow-y: auto;
      }
      .reveal .columns {
        display: flex;
        align-items: flex-start;
      }
      .reveal .column {
        flex: 1;
        margin: 0 1em;
      }
      .reveal img {
        max-width: 100%;
        max-height: 70vh;
        object-fit: contain;
      }
      .center {
        text-align: center;
      }
      .fragment {
        opacity: 0.3;
      }
      .fragment.visible {
        opacity: 1;
      }
---

# Getting Started with PyMC v5 {.center}

::: {.center}
<div style="font-size: 4em; margin-bottom: 0.5em; margin-top: 1em;">
  üêç + üìä + üß† = PyMC
</div>

<div style="font-size: 1.5em; color: #888; margin-bottom: 2em;">
  Data Umbrella Tutorial
</div>

<div style="font-size: 1.2em; margin-top: 2em;">
  Christopher Fonnesbeck<br>
  <span style="font-size: 0.9em; color: #888;">PyMC Labs</span>
</div>
:::

---

## What We'll Learn Today

::: {style="font-size: 0.9em; margin-top: 1em;"}
- üß† **Bayesian Thinking** - Understanding uncertainty
- üìä **Data Analysis** - Real-world examples  
- üîß **Practical Skills** - Building real models
- üöÄ **Best Practices** - Avoiding pitfalls
:::

::: {.notes}
This tutorial is designed to be practical and hands-on. We'll start with the basics but quickly move to real examples.

The four key areas we'll focus on:

1. Bayesian Thinking: Understanding how to think in terms of uncertainty, priors, and updating beliefs with data.

2. Practical Skills: You'll learn to write PyMC code, run MCMC samplers, and interpret results.

3. Data Analysis: We'll work with actual datasets and solve real problems, not just toy examples.

4. Best Practices: I'll share common mistakes I see in the field and how to avoid them.

This isn't just about learning PyMC syntax - it's about becoming a better data scientist who can handle uncertainty properly.
:::

## What is PyMC?

::: {.center}
<div style="font-size: 4em; margin-bottom: 0.5em; margin-top: 1em;">
  üêç + üìä + üß† = PyMC
</div>

<div style="font-size: 1.5em; color: #888;">
  Python + Statistics + Probabilistic Thinking
</div>
:::

::: {.notes}
PyMC is where Python meets probabilistic programming.

It's fundamentally different from traditional statistical packages:
- Instead of point estimates, we work with distributions
- Instead of p-values, we get credible intervals
- Instead of assuming our models are correct, we quantify our uncertainty

PyMC makes Bayesian modeling accessible by providing:
- Intuitive model specification using Python syntax
- Automatic differentiation for efficient computation
- State-of-the-art MCMC algorithms (especially NUTS)
- Seamless integration with the Python data science ecosystem
- Excellent visualization tools through ArviZ

Think of it as a way to build models that explicitly handle uncertainty at every step.
:::

## Why Bayesian Modeling?

::: {.columns}
::: {.column width="50%"}
**üìà Traditional Approach**

- Point estimates
- p-values  
- Confidence intervals*
- Null hypothesis testing
:::

::: {.column width="50%"}
**üéØ Bayesian Approach**

- Full distributions
- Probability statements
- Credible intervals
- Direct inference
:::
:::

::: {.notes}
The Bayesian approach is fundamentally more intuitive and informative:

Traditional Statistics:
- Gives you point estimates with error bars
- p-values are confusing (what's the probability that the null hypothesis is true? NOT what p-values tell you!)
- Confidence intervals have a weird interpretation: "If we repeated this study 100 times, 95% of the confidence intervals would contain the true value"
- Focuses on rejecting null hypotheses rather than quantifying effects

Bayesian Statistics:
- Gives you the full distribution of possible parameter values
- You can make direct probability statements: "There's a 95% chance the effect is between X and Y"
- Credible intervals have the intuitive interpretation you want
- You can incorporate prior knowledge naturally
- You get uncertainty quantification for free

The Bayesian approach answers the questions you actually want to ask about your data.
:::

## Real-World Applications

::: {.columns}
::: {.column width="50%"}
**üè• Healthcare**
- Clinical trial design
- Drug dosing optimization  
- Epidemiological modeling

**üß¨ Science**
- Parameter estimation
- Model comparison
- Experimental design
:::

::: {.column width="50%"}
**üí∞ Finance**
- Risk modeling
- Portfolio optimization
- Credit scoring

**üì± Tech**
- A/B testing
- Recommendation systems
- Anomaly detection
:::
:::

::: {.notes}
PyMC is used across many industries because uncertainty is everywhere:

Healthcare:
- Clinical trials need to account for patient variability
- Drug dosing must be personalized based on patient characteristics
- Epidemiological models help track disease spread with uncertainty
- Diagnostic tests have false positive/negative rates that need proper handling

Finance:
- Risk models must quantify uncertainty in potential losses
- Portfolio optimization needs to account for parameter uncertainty
- Credit scoring benefits from hierarchical models for different populations
- Market forecasting explicitly acknowledges we don't know the future

Science:
- Parameter estimation with proper uncertainty quantification
- Model comparison using information criteria and cross-validation
- Experimental design optimizes information gain
- Meta-analysis combines evidence from multiple studies

Technology:
- A/B testing with proper statistical inference
- Recommendation systems that adapt to user preferences
- Anomaly detection that adapts to changing baselines
- User behavior models that capture individual differences

The common thread: all these applications involve uncertainty that needs to be properly quantified and propagated.
:::

# üì¶ Installation & Setup

::: {.center}
<div style="font-size: 3em; margin-top: 1em;">
  Let's get you up and running!
</div>
:::

::: {.notes}
Before we dive into modeling, let's make sure everyone has PyMC properly installed and configured.

There are a few different ways to install PyMC, and the choice depends on your setup and needs. I'll show you the recommended approaches and help you troubleshoot common issues.

Don't worry if you run into problems - PyMC has some complex dependencies, and installation issues are common. We'll work through them together.
:::

## üöÄ Recommended Installation

::: {.center}
<div style="font-family: monospace; font-size: 2.5em; margin-bottom: 0.5em;">
  conda install -c conda-forge pymc
</div>

<div style="font-size: 1.1em; color: #888; margin-bottom: 1em;">
  (conda-forge is the official recommended method)
</div>

<div style="font-size: 1.1em;">
  ‚úÖ Best dependency management<br>
  ‚úÖ Includes ArviZ automatically<br>
  ‚úÖ Most stable installation
</div>
:::

::: {.notes}
The PyMC team officially recommends conda-forge for installation:

```bash
# Create a new environment (recommended)
conda create -c conda-forge -n pymc_env "pymc>=5"
conda activate pymc_env
```

Or install in existing environment:
```bash
conda install -c conda-forge pymc
```

Why conda-forge is recommended:
- Better handling of complex numerical dependencies (BLAS, LAPACK)
- Pre-compiled binaries avoid compilation issues
- Consistent versions across the numerical Python stack
- Official PyMC recommendation per documentation

For advanced users who need cutting-edge features:
```bash
# Development installation in existing conda env
pip install -e .  # Only for contributors
```

Never use regular conda channel - always use conda-forge.
:::

## Sampling Backends & Libraries

::: {.columns}
::: {.column width="50%"}
**üîß PyTensor (Default)**
- CPU-based, most stable
- Works everywhere  
- All PyMC features supported

**‚ö° JAX + NumPyro**
- GPU/TPU acceleration
- `conda install numpyro`
- Some limitations
:::

::: {.column width="50%"}
**üöÄ Nutpie**
- Rust + Numba performance
- `conda install -c conda-forge nutpie`
- CPU-optimized NUTS

**‚ö´ BlackJAX**
- JAX-based samplers
- `conda install blackjax`
- Advanced MCMC methods
:::
:::

::: {.notes}
PyMC supports multiple computational backends and sampling libraries:

1. PyTensor Backend (Default):
   - CPU-based using NumPy/SciPy
   - Most stable and battle-tested
   - Works on any system with Python
   - All PyMC features fully supported
   - Best choice for learning and most applications

2. JAX Backend with NumPyro:
   - GPU and TPU acceleration
   - Install: conda install numpyro
   - 5-10x faster for large models
   - JIT compilation for speed
   - Some PyMC features may have limitations

3. Nutpie (Fast CPU sampling):
   - Rust-based NUTS implementation with Numba
   - Install: conda install -c conda-forge nutpie
   - Optimized for CPU performance
   - Faster than default PyTensor for many models

4. BlackJAX (Advanced JAX samplers):
   - JAX-based sampling library
   - Install: conda install blackjax
   - Advanced MCMC algorithms
   - Useful for research and specialized sampling

For this tutorial, we'll use the default PyTensor backend since it works reliably for everyone and handles all our examples perfectly.

You can explore faster backends later when working with larger, more complex models.
:::

## ‚úÖ Test Your Installation

```{.python code-line-numbers="true"}
import pymc as pm
print(f"PyMC version: {pm.__version__}")

import arviz as az
print(f"ArviZ version: {az.__version__}")

# Quick test
x = pm.Normal.dist(mu=0, sigma=1)
print(f"Test sample: {pm.draw(x, draws=3)}")
```

::: {.fragment}
<div style="font-size: 1.5em; margin-top: 1em; text-align: center; color: #4caf50;">
  üéâ If this runs, you're ready!
</div>
:::

::: {.notes}
Let's verify your installation works correctly.

First, check versions:
- PyMC version should be 5.x.x (we need version 5)
- ArviZ version should be 0.15+ for best compatibility

Then run a quick test to make sure the basic functionality works:
- Create a simple Normal distribution
- Draw some samples from it
- If you see an array of random numbers, everything is working!

Common issues and solutions:

If you get import errors:
- Try: pip install --upgrade pymc arviz
- Or: conda update pymc arviz

If you get compilation errors:
- On Windows: install Microsoft Visual C++ Build Tools
- On Mac: install Xcode command line tools (xcode-select --install)
- On Linux: install build-essential package

If sampling is very slow:
- This is normal for the first run (compilation overhead)
- Subsequent runs should be much faster
:::

## üõ†Ô∏è Development Setup

::: {.columns}
::: {.column width="50%"}
**üìì Jupyter Notebooks**  
*Recommended for learning*

- ‚úÖ Interactive exploration
- ‚úÖ Immediate plot display  
- ‚úÖ Great for tutorials
:::

::: {.column width="50%"}
**üÜö VS Code + Jupyter**  
*Best of both worlds*

- ‚úÖ Code completion
- ‚úÖ Git integration
- ‚úÖ Notebook support
:::
:::

::: {.notes}
For Bayesian modeling, your development environment matters:

Jupyter Notebooks:
- Perfect for exploratory data analysis
- Interactive plots work seamlessly
- Easy to iterate on models
- Great for sharing results
- ArviZ plots display beautifully
- Recommended for learning PyMC

VS Code:
- Better for production code
- Excellent debugging capabilities
- Smart code completion for PyMC
- Integrated version control
- Can run notebooks natively
- Better for larger projects

Other good options:
- PyCharm: Great IDE features, good for complex projects
- Google Colab: Free GPU access, good for large models
- Spyder: Popular in scientific computing

For this tutorial, either Jupyter or VS Code will work great. Choose what you're most comfortable with.

Pro tip: Start with Jupyter for exploration, then move to VS Code when you want to turn your analysis into reusable code.
:::

## üîß Troubleshooting Common Issues

**üí• Import Errors**
```bash
conda update -c conda-forge pymc arviz
pip install --upgrade pymc arviz
```

**‚ö†Ô∏è Windows g++ Compiler Warning**
```bash
conda install m2w64-toolchain
```
*Fixes severe performance degradation*

**üñ•Ô∏è Platform Notes**
- **Mac M1/M2:** Use conda-forge only  
- **Linux:** Generally smooth  
- **Colab:** May need runtime restart

**üîç Need Help?** ‚Üí discourse.pymc.io

::: {.notes}
Installation issues are the most common beginner pain points - here's what actually works:

Windows g++ Compiler Warning:
- Single most frequent issue: "WARNING (pytensor.configdefaults): g++ not available"
- Causes severe performance degradation (10-100x slower)
- Solution: conda install m2w64-toolchain in your PyMC environment
- This is a PyMC-specific issue, not general Python

Platform-Specific Issues:
- Mac M1/M2: BLAS library conflicts, avoid MKL packages, use conda-forge exclusively
- Linux: Generally smoothest installation experience
- Google Colab: Pre-installed packages conflict, may need runtime restart after pip install

Universal Best Practice:
- Always create dedicated environment: conda create -c conda-forge -n pymc_env "pymc>=5" python=3.10
- PyMC's complex dependency chain requires conda-forge for reliable installation
- pip installations frequently fail with BLAS or Unicode issues

Import/Version Errors:
- Try: conda update -c conda-forge pymc arviz
- If persistent: create fresh environment
- Never use regular conda channel - always conda-forge

Getting Help:
- discourse.pymc.io is extremely beginner-friendly
- Include full error messages and environment info
- Most installation issues have been solved before
:::

# üß† PyMC Fundamentals

::: {.center}
<div style="font-size: 3em; margin-top: 1em;">
  The building blocks of Bayesian models
</div>
:::

::: {.notes}
Now that we have PyMC installed, let's understand how it works.

PyMC has a few core concepts that are essential to understand:
- Model containers that hold everything together
- Random variables that represent unknown quantities
- Distributions that encode our assumptions
- Observed data that updates our beliefs

We'll start simple and build up complexity gradually. By the end of this section, you'll understand how PyMC thinks about statistical models.
:::

## The Model Container

::: {.center}
<div style="font-size: 4em; margin-bottom: 0.5em;">
  üì¶
</div>
:::

```{.python code-line-numbers="1|2-3|5-6"}
with pm.Model() as model:
    # All random variables go here
    mu = pm.Normal("mu", mu=0, sigma=10)
    
    # PyMC tracks relationships automatically
    x = pm.Normal("x", mu=mu, sigma=1, observed=data)
```

::: {.notes}
The Model container is PyMC's way of organizing your statistical model.

Key points:
- Uses Python's `with` statement (context manager)
- Everything defined inside becomes part of the model
- PyMC automatically tracks variable relationships
- Variables must have unique names (strings)
- The model builds a computational graph behind the scenes

Think of it like a recipe:
- The model is your cookbook
- Each variable is an ingredient
- PyMC figures out how to combine everything

You can have multiple models in the same script, each with their own variables and parameters.

The model context is where the magic happens - PyMC builds a directed acyclic graph (DAG) that represents all the probabilistic relationships in your model.
:::

## Random Variables & Distributions

```{.python code-line-numbers="1-2|4-5|7-8"}
# Unobserved (parameters to estimate)
mu = pm.Normal("mu", mu=0, sigma=10)
sigma = pm.HalfNormal("sigma", sigma=1)

# Deterministic transformations
scaled_mu = pm.Deterministic("scaled_mu", mu * 2)

# Observed (your data)
y = pm.Normal("y", mu=mu, sigma=sigma, observed=data)
```

::: {.notes}
PyMC has three main types of variables:

1. Unobserved Random Variables (Parameters):
   - These are what you want to estimate
   - Have prior distributions that encode your beliefs
   - Example: mu ~ Normal(0, 10) says we think mu is around 0, but could be as far as ¬±20 with reasonable probability

2. Deterministic Variables:
   - Functions of other variables
   - No additional randomness
   - Useful for transformations and derived quantities
   - Example: scaled_mu = mu * 2

3. Observed Variables (Data):
   - Your actual data
   - Fixed during inference
   - Connected to parameters through the likelihood
   - Example: y ~ Normal(mu, sigma) where y is your observed data

The key insight: you're building a generative model. You're saying "this is how I think the data was generated" and then inverting that process to learn about the parameters.
:::

## Distribution Zoo ü¶Å

![](/public/plots/distributions/distribution_zoo.png){fig-align="center"}

::: {.notes}
PyMC provides a comprehensive set of probability distributions:

Continuous Distributions:
- Normal: The workhorse, symmetric, unbounded
- Beta: For probabilities and proportions (0 to 1)
- Exponential: For positive values, waiting times
- Gamma: Positive values with more flexibility than exponential
- Student-t: Like normal but with heavier tails
- Many more: Uniform, LogNormal, Cauchy, etc.

Discrete Distributions:
- Poisson: Count data, events per time period
- Binomial: Success/failure with fixed number of trials
- Categorical: Multiple discrete outcomes
- Bernoulli: Single success/failure trial

Choosing distributions:
- Match the support to your data (positive, bounded, etc.)
- Consider the shape you expect
- Start simple (Normal is often a good default)
- Use domain knowledge when available

Each distribution has parameters that control its shape and location. Understanding these parameters is key to building good models.
:::

## Observed Data

::: {.columns}
::: {.column width="60%"}
```{.python code-line-numbers="1-2|4-5|6-7|8"}
# Your actual data
data = np.array([1.2, 2.3, 1.8, 2.9, 3.1])

with pm.Model() as model:
    mu = pm.Normal("mu", mu=0, sigma=10)
    sigma = pm.HalfNormal("sigma", sigma=1)
    observations = pm.Normal("obs", mu=mu, 
                           sigma=sigma, observed=data)
```
:::

::: {.column width="40%"}
<div style="margin-top: 2em;">
  <div style="font-size: 1.5em; margin-bottom: 1em;">Key Points</div>
  <ul style="font-size: 0.9em;">
    <li>üéØ <code>observed=data</code> makes it a likelihood</li>
    <li>üìä Data shape must match distribution</li>
    <li>üîó Links parameters to your actual data</li>
    <li>‚ö° This is where Bayes' theorem happens!</li>
  </ul>
</div>
:::
:::

::: {.notes}
The `observed` argument is where your data meets your model:

What happens with observed=data:
- Tells PyMC these values are fixed and known
- Creates the likelihood function automatically
- Links your parameters (mu, sigma) to your actual observations
- This is where Bayes' theorem gets applied

Important considerations:
- Data shape must match what the distribution expects
- Missing data (NaN) is handled automatically
- You can have multiple observed variables
- The observed variable name is just for reference

Behind the scenes:
- PyMC computes log P(data | parameters) for each MCMC sample
- Combined with priors P(parameters), this gives you the posterior
- The observed data never changes during sampling
- Only the parameters (mu, sigma) are updated

This is the heart of Bayesian inference: updating our beliefs about parameters based on observed data.
:::

## üìä Data Handling Pitfalls

**‚ùå Wrong: Pandas inside model**
```python
with pm.Model():
    means = df.groupby('category').mean()  # This fails!
```

**‚úÖ Correct: Prepare data first**
```python
means = df.groupby('category').mean().values
with pm.Model():
    data = pm.Data("data", means)
```

**üîß Key Patterns:**
- Complete pandas operations **outside** model context
- Convert to numpy arrays before entering model
- Use `pm.Data()` for data that might change
- Use `pt.where()` for missing data (JAX/NumPyro compatible)

::: {.notes}
Data preparation errors reflect fundamental misunderstandings about PyMC's computational model:

The Core Problem:
- PyMC operates on symbolic tensors, not pandas DataFrames
- Most pervasive mistake: using pandas operations inside model context
- df.groupby('category').mean() inside with pm.Model() fails
- PyMC needs fixed numpy arrays or tensor operations

The Solution Pattern:
1. Complete ALL pandas operations outside the model context
2. Convert results to numpy arrays (.values)
3. Use pm.Data() for data that needs updating
4. Enter the model context only with prepared tensors

Missing Data Handling:
- JAX/NumPyro backends don't support boolean masks
- Use pt.where() for conditional operations (works with all backends)
- Or explicitly model missing values as latent variables
- Avoid numpy-style masking operations

Categorical Variables:
- Don't one-hot encode everything for PyMC
- Use pd.factorize() for index-based encoding
- Combine with coordinate systems for meaningful labels  
- More efficient and scales better for hierarchical models

Best Practice Workflow:
data_prep.py -> clean pandas operations
model.py -> pure PyMC tensor operations
This separation prevents most data handling errors.
:::

## The Bayesian Recipe

::: {.center}
<div style="font-size: 2.5em; margin-bottom: 1em;">
  Prior √ó Likelihood = Posterior
</div>
:::

```{.python code-line-numbers="2-3|5-6|8"}
with pm.Model() as model:
    # Prior: What we believe before seeing data
    mu = pm.Normal("mu", mu=0, sigma=10)
    
    # Likelihood: How we think data was generated
    y = pm.Normal("y", mu=mu, sigma=1, observed=data)
    
    # Posterior: Updated beliefs (computed by MCMC)
```

::: {.notes}
This is the heart of Bayesian statistics:

Prior:
- Your beliefs before seeing the data
- Can be informative (based on domain knowledge) or uninformative (letting data dominate)
- Example: mu ~ Normal(0, 10) says we think mu is probably near 0

Likelihood:
- Your model of how the data was generated
- Given parameters, what's the probability of observing your data?
- Example: y ~ Normal(mu, 1) says each observation comes from a normal distribution

Posterior:
- Your updated beliefs after seeing the data
- Combines prior knowledge with information from data
- This is what MCMC sampling gives you
- Automatically balances prior and likelihood based on data quality

Bayes' theorem: P(parameters | data) ‚àù P(data | parameters) √ó P(parameters)

The beauty: everything is probabilistic, so you get full uncertainty quantification for free.
:::

## PyMC ‚ù§Ô∏è ArviZ Integration

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üé® PyMC</div>
<ul style="font-size: 1.1em;">
  <li>Model specification</li>
  <li>MCMC sampling</li>
  <li>Prior/posterior predictive</li>
  <li>Model comparison</li>
</ul>
:::

::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üìä ArviZ</div>
<ul style="font-size: 1.1em;">
  <li>Visualization</li>
  <li>Diagnostics</li>
  <li>Model checking</li>
  <li>Summary statistics</li>
</ul>
:::
:::

::: {.center}
<div style="font-size: 1.1em; color: #888; margin-top: 2em;">
  Seamless workflow: PyMC ‚Üí InferenceData ‚Üí ArviZ
</div>
:::

::: {.notes}
PyMC and ArviZ work together seamlessly:

PyMC handles:
- Defining your statistical model
- Running MCMC sampling (NUTS algorithm)
- Prior and posterior predictive sampling
- Model comparison metrics (LOO, WAIC)

ArviZ provides:
- Publication-quality plots
- Comprehensive diagnostics
- Model checking tools
- Summary statistics and tables

The connection is through InferenceData:
- A standardized format for Bayesian analysis results
- Contains posterior samples, prior samples, observed data, etc.
- Works with PyMC, Stan, TensorFlow Probability, and more
- Makes your analysis reproducible and shareable

Typical workflow:
1. Define model in PyMC
2. Sample with pm.sample() ‚Üí InferenceData object
3. Visualize and diagnose with ArviZ
4. Iterate and improve your model

This separation of concerns lets each tool focus on what it does best.
:::

# üèóÔ∏è Building Your First Model

::: {.center}
<div style="font-size: 1.8em; margin-top: 0.5em;">
  From data to insights with real examples
</div>
:::

::: {.notes}
Now comes the fun part - building and fitting actual Bayesian models!

We'll work through a complete example from start to finish:
- Understand the problem and data
- Specify a Bayesian model
- Check our priors make sense
- Sample from the posterior
- Diagnose potential issues
- Interpret results
- Make predictions

This isn't a toy example - we'll use real data and go through all the steps you'd follow in practice.
:::

## üß™ The Bioassay Problem

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">The Question</div>
<div style="font-size: 1.1em;">
  How does drug dose affect mortality in lab animals?
</div>
:::

::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">The Data</div>
<div style="font-size: 1.1em; margin-top: 1em;">
  <div><strong>Dose:</strong> [-0.86, -0.3, -0.05, 0.73]</div>
  <div><strong>Animals:</strong> [5, 5, 5, 5]</div>
  <div><strong>Deaths:</strong> [0, 1, 3, 5]</div>
</div>
:::
:::

::: {.center}
<div style="font-size: 1.1em; color: #888; margin-top: 2em;">
  Classic dose-response modeling problem
</div>
:::

::: {.notes}
This is a classic problem in toxicology and pharmacology:

The Setup:
- We have 4 different dose levels (on log scale, centered)
- 5 animals tested at each dose
- We count how many animals died at each dose
- Goal: model the dose-response relationship

Why this matters:
- Understand drug safety and efficacy
- Determine safe dosage ranges
- Predict effects at untested doses
- Quantify uncertainty in our predictions

The Data:
- Dose -0.86: 0 out of 5 animals died (0% mortality)
- Dose -0.3: 1 out of 5 animals died (20% mortality)
- Dose -0.05: 3 out of 5 animals died (60% mortality)
- Dose 0.73: 5 out of 5 animals died (100% mortality)

Clear dose-response relationship, but we want to model this probabilistically to:
- Get uncertainty estimates
- Make predictions for new doses
- Understand the shape of the dose-response curve
:::

## Building the Model

```{.python code-line-numbers="1-4|6-7|9-10|11-12|14-15"}
# The data
dose = np.array([-0.86, -0.3, -0.05, 0.73])
n_animals = np.array([5, 5, 5, 5])
n_deaths = np.array([0, 1, 3, 5])

with pm.Model() as bioassay_model:
    # Priors for intercept and slope
    alpha = pm.Normal('alpha', mu=0, sigma=2.5)
    beta = pm.Normal('beta', mu=0, sigma=2.5)
    
    # Logistic regression model
    theta = pm.invlogit(alpha + beta * dose)
    
    # Binomial likelihood
    deaths = pm.Binomial('deaths', n=n_animals, p=theta, observed=n_deaths)
```

::: {.notes}
Let's build this step by step:

The Data:
- dose: log-transformed and centered doses
- n_animals: number of animals at each dose (5 each)
- n_deaths: observed deaths at each dose

The Model Structure:

1. Priors:
   - alpha: intercept (log-odds at dose=0)
   - beta: slope (how log-odds change with dose)
   - Normal(0, 2.5) is weakly informative - allows wide range but prevents extreme values

2. Linear Predictor:
   - alpha + beta * dose gives log-odds of death
   - This is the standard logistic regression setup

3. Inverse Logit Transformation:
   - Converts log-odds to probabilities (0 to 1 range)
   - theta[i] = probability of death at dose[i]

4. Likelihood:
   - Binomial distribution: n_deaths ~ Binomial(n_animals, theta)
   - This says: at each dose, deaths follow a binomial distribution
   - Like flipping n_animals coins, each with probability theta of "death"

This is a generalized linear model (GLM) with logit link - a Bayesian version of logistic regression.
:::

## ‚ö†Ô∏è Common Modeling Errors

::: {.columns}
::: {.column width="50%"}
<div style="background-color: #c62828; padding: 1em; border-radius: 0.5em;">
  <div style="color: #ffcdd2; font-weight: bold; margin-bottom: 0.5em;">üî¢ Shape Mismatches</div>
  <div style="font-size: 0.9em; color: #ffcdd2;">
    <code>ValueError: Input dimension mis-match</code><br>
    Matrix vs element-wise operations<br>
    Explicit shape specification needed
  </div>
</div>
:::

::: {.column width="50%"}
<div style="background-color: #ff6f00; padding: 1em; border-radius: 0.5em;">
  <div style="color: #fff3e0; font-weight: bold; margin-bottom: 0.5em;">üìä Broadcasting Issues</div>
  <div style="font-size: 0.9em; color: #fff3e0;">
    Mixing <code>shape</code> and <code>dims</code><br>
    Inconsistent coordinate systems<br>
    Check tensor shapes explicitly
  </div>
</div>
:::
:::

```python
# ‚ùå Wrong: element-wise when you want matrix multiplication  
mu = X * beta  

# ‚úÖ Correct: explicit matrix operations
beta = pm.Normal('beta', mu=0, sd=1, shape=(n_features,))
mu = pm.math.dot(X, beta)
```

::: {.notes}
Shape mismatches are the most common modeling error after installation:

Matrix Multiplication Confusion:
- Beginners confuse * (element-wise) with matrix multiplication
- ValueError: Input dimension mis-match is the telltale sign
- Always be explicit: use pm.math.dot() for matrix operations
- Specify shapes explicitly: shape=(n_features,) not just scalar

Broadcasting Problems:
- PyMC has strict broadcasting rules unlike NumPy
- Don't mix shape and dims parameters inconsistently
- Understand positional broadcasting vs named dimensions
- When in doubt, check tensor shapes with .eval()

Best Practices:
- Always specify shapes for coefficient vectors: shape=(n_features,)
- Use pm.math functions instead of numpy equivalents inside models
- Test with simple data first to catch shape errors early
- Be explicit about broadcasting operations

The pattern: beta with shape=(n_features,) and pm.math.dot(X, beta) works reliably for linear models.
:::

## Prior Predictive Check

![](/public/plots/bioassay/prior_predictive.png){fig-align="center"}

::: {.notes}
Before fitting the model, let's check if our priors make sense:

What we're looking at:
- Each line shows a possible dose-response curve under our priors
- X-axis: dose (log scale)
- Y-axis: probability of death (0 to 1)

What we want to see:
- Curves that cover reasonable ranges
- Mostly monotonic relationships (higher dose ‚Üí higher death probability)
- No extreme or impossible behaviors

What we observe:
- Good variety of curves from gentle to steep
- Most are monotonically increasing (good!)
- Some flat or slightly decreasing curves (that's okay - priors should be somewhat agnostic)
- Probability ranges from 0 to 1 as expected

This looks reasonable! Our priors allow for a wide range of dose-response relationships without being too restrictive.

If we saw problems (e.g., all curves were flat, or probabilities went outside [0,1]), we'd need to adjust our priors.

Prior predictive checks are crucial - they catch modeling mistakes before you waste time on sampling.
:::

## Sampling the Posterior

```{.python code-line-numbers="1-2|4-5"}
with bioassay_model:
    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42)

# PyMC automatically:
# ‚úì Chose NUTS sampler    ‚úì Tuned parameters
# ‚úì Ran 4 parallel chains ‚úì Checked convergence
```

::: {.center}
<div style="font-size: 1.5em; margin-top: 1em;">
  ‚ö° Modern MCMC is (mostly) automatic! ‚ö°
</div>
:::

::: {.notes}
Time to fit the model using MCMC sampling:

What pm.sample() does:
- Automatically selects NUTS (No-U-Turn Sampler) - state of the art
- Runs a tuning phase to find good step sizes and mass matrix
- Samples from the posterior using 4 parallel chains
- Monitors convergence diagnostics
- Returns an InferenceData object with all results

Parameters explained:
- 2000: number of posterior samples per chain (after tuning)
- tune=1000: number of tuning/warmup samples (discarded)
- chains=4: number of parallel chains (helps detect convergence issues)
- random_seed=42: for reproducibility

What happens during sampling:
1. Tuning phase (1000 steps): NUTS learns about the posterior geometry
2. Sampling phase (2000 √ó 4 = 8000 total samples): collect posterior samples
3. Convergence checks: R-hat, effective sample size, divergences

Modern MCMC is remarkably robust:
- NUTS automatically adapts to the posterior shape
- Parallel chains help identify problems
- Comprehensive diagnostics catch most issues
- Most models "just work" without manual tuning

This is a huge advance over older MCMC methods that required lots of manual tuning.
:::

## Trace Plots: Checking Convergence

![](/public/plots/bioassay/trace_plot.png){fig-align="center"}

::: {.notes}
Trace plots are your first check for sampling problems:

What we're looking at:
- Left panels: posterior distributions (what we care about)
- Right panels: trace plots showing parameter values over MCMC iterations

What we want to see:
- "Fuzzy caterpillars" - traces should look like random noise
- All chains (different colors) should overlap and explore the same space
- No obvious trends, patterns, or getting stuck
- Smooth, unimodal posterior distributions

What we observe:
- Both alpha and beta show excellent mixing
- All 4 chains are exploring the same regions
- No signs of convergence problems
- Posterior distributions look smooth and reasonable

Signs of problems to watch for:
- Chains that don't overlap (convergence failure)
- Trends in the traces (not stationary)
- Chains getting stuck in one region
- Very different behavior between chains

Our traces look great! This suggests our MCMC sampling worked well and we can trust our posterior estimates.
:::

## Posterior Distributions

![](/public/plots/bioassay/posterior_plot.png){fig-align="center"}

::: {.notes}
Now let's look at what we learned about the parameters:

Alpha (Intercept):
- Mean around 0.24
- Represents log-odds of death at dose = 0 (center of dose range)
- Exp(0.24) / (1 + exp(0.24)) ‚âà 56% probability of death at average dose
- Uncertainty: 95% credible interval roughly [-1, 1.4]

Beta (Slope):
- Mean around 4.03
- Represents how log-odds change per unit increase in dose
- Strong positive relationship: higher dose ‚Üí higher death probability
- This is a steep dose-response curve
- Uncertainty: 95% credible interval roughly [1.5, 6.9]

Key insights:
- There's definitely a dose-response relationship (beta > 0 with high confidence)
- But there's still uncertainty about the exact slope
- At the lowest dose, death probability is low but not zero
- At the highest dose, death probability is very high

Compare to classical statistics:
- Instead of point estimates + standard errors, we get full distributions
- Instead of p-values, we can make direct probability statements
- "There's a 97.5% chance beta is greater than 1.5" - much more intuitive!
:::

## Parameter Relationships

![](/public/plots/bioassay/pair_plot.png){fig-align="center"}

::: {.notes}
The pair plot shows how parameters relate to each other:

What we're looking at:
- Scatter plot of alpha vs beta posterior samples
- Each point represents one MCMC draw
- Contours show the joint posterior density

What we observe:
- Slight negative correlation between alpha and beta
- This makes sense: if the intercept is higher, the slope can be lower to fit the same data
- The correlation isn't too strong (good for inference)
- Joint distribution looks well-behaved (elliptical, unimodal)

Why parameter correlations matter:
- Strong correlations can slow MCMC sampling
- Indicate potential identifiability issues
- Affect prediction uncertainty
- Can suggest model reparameterizations

In our case:
- Correlation is mild and not problematic
- Both parameters are well-identified by the data
- The relationship makes biological sense

This is another sign that our model and sampling worked well. Strong correlations or weird shapes here would suggest problems.
:::

## Model Summary

```python
az.summary(trace, var_names=['alpha', 'beta'])
```

```
        mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
alpha  0.241  0.634  -0.985    1.430      0.010    0.008    4266.0    4089.0    1.0
beta   4.034  1.447   1.532    6.869      0.022    0.018    5094.0    4724.0    1.0
```

::: {.center}
<div style="font-size: 1.5em;">üéØ Excellent diagnostics!</div>
:::

::: {.notes}
The summary table gives us key information about our posterior and sampling:

Key Statistics:
- mean: posterior mean (point estimate)
- sd: posterior standard deviation (uncertainty)
- hdi_3%, hdi_97%: 94% highest density interval (credible interval)

Diagnostics:
- mcse_mean, mcse_sd: Monte Carlo standard error (should be small)
- ess_bulk, ess_tail: effective sample size (want > 400, ideally > 1000)
- r_hat: potential scale reduction factor (want < 1.01, ideally 1.00)

Our Results:
- Both parameters well-estimated with reasonable uncertainty
- HDI intervals don't include extreme values
- Excellent effective sample sizes (4000+)
- Perfect R-hat values (1.0)
- Low Monte Carlo error

What this means:
- Our MCMC sampling was very efficient
- We have enough samples for reliable inference
- No convergence issues detected
- We can trust our posterior estimates

This is what good Bayesian inference looks like - clean diagnostics and interpretable results.
:::

## Posterior Predictive Check

![](/public/plots/bioassay/posterior_predictive.png){fig-align="center"}

::: {.notes}
The posterior predictive check validates our model against reality:

What we're looking at:
- Red points: observed data (actual death rates at each dose)
- Blue lines: predicted dose-response curves from posterior samples
- Yellow line: mean prediction across all posterior samples

What we want to see:
- Model predictions that capture the observed data
- Reasonable uncertainty bands around predictions
- No systematic deviations between model and data

What we observe:
- Model captures the dose-response trend very well
- Observed data points fall within the prediction envelope
- Smooth S-shaped curve characteristic of logistic regression
- Appropriate uncertainty - wider where we have less data

Model validation:
- At dose -0.86: model predicts low death probability (‚úì matches 0/5 deaths)
- At dose 0.73: model predicts high death probability (‚úì matches 5/5 deaths)
- Intermediate doses show reasonable predictions

This suggests our model is a good fit to the data and can be trusted for making predictions at new dose levels.

If we saw systematic deviations, we'd need to consider:
- Different link functions
- Non-linear dose effects
- Individual animal variability (hierarchical modeling)
:::

## Making Predictions

```{.python code-line-numbers="1-2|4-5|7-8"}
# Predict mortality at new doses
new_doses = np.array([-1.0, 0.0, 1.0])

with bioassay_model:
    pm.set_data({'dose': new_doses})
    posterior_pred = pm.sample_posterior_predictive(trace)

# Get prediction intervals
pred_mortality = posterior_pred.posterior_predictive['deaths']
```

::: {.center}
<div style="font-size: 1.5em; margin-top: 1em;">
  üîÆ Full uncertainty quantification for free!
</div>
:::

::: {.notes}
One of the biggest advantages of Bayesian modeling - making predictions with uncertainty:

The Process:
1. Define new dose values we want predictions for
2. Use pm.set_data() to update the model with new doses (requires dose to be defined with pm.Data)
3. Sample from posterior predictive distribution
4. Get full distribution of possible outcomes

What we get:
- Not just point predictions, but full distributions
- Credible intervals for mortality rates
- Probability of any specific outcome
- Proper uncertainty propagation from parameter uncertainty

Example interpretations:
- "At dose -1.0, we predict 0-2 deaths out of 5 animals with 95% probability"
- "At dose 1.0, we predict 4-5 deaths out of 5 animals with 95% probability"
- "The median predicted mortality at dose 0.0 is 60%"

Compared to classical approaches:
- Classical: point prediction ¬± standard error
- Bayesian: full predictive distribution
- Can answer any question: "What's the probability that fewer than 2 animals die?"

This uncertainty quantification is crucial for decision-making in pharmaceutical and toxicology applications.
:::

# ‚ö†Ô∏è Common Pitfalls & Solutions

::: {.center}
<div style="font-size: 1.8em; margin-top: 0.5em;">
  Avoiding the traps that catch beginners
</div>
:::

::: {.notes}
Even with modern tools like PyMC, Bayesian modeling can go wrong. Let me share the most common issues I see and how to fix them.

These aren't theoretical problems - these are real issues that will happen to you as you build more complex models. Learning to recognize and fix them is crucial for successful Bayesian analysis.

We'll cover:
- Convergence failures and how to diagnose them
- Divergences and what they mean for your results
- Performance issues and optimization strategies
- Prior specification problems
- Model checking failures
:::

## Convergence Diagnostics

::: {.columns}
::: {.column width="50%"}
**üìà R-hat < 1.01**  
*Chains have converged*

**üìä ESS > 400**  
*Enough effective samples*
:::

::: {.column width="50%"}
**‚ö†Ô∏è Zero Divergences**  
*No numerical issues*

**üîó Good Mixing**  
*Fuzzy caterpillars in traces*
:::
:::

::: {.notes}
Always check these four diagnostics before trusting your results:

R-hat (Potential Scale Reduction Factor):
- Compares within-chain vs between-chain variance
- Should be < 1.01 for all parameters
- > 1.01 means chains haven't converged to the same distribution
- Much > 1.1 is a serious problem

Effective Sample Size (ESS):
- How many independent samples you effectively have
- Accounts for autocorrelation in MCMC chains
- Need ESS > 400 for basic inference, > 1000 is better
- Low ESS means high autocorrelation (slow mixing)

Divergences:
- Numerical instabilities during sampling
- Even a few divergences can bias results
- Often indicate problems with model specification or geometry
- Zero divergences is the goal

Chain Mixing:
- Visual inspection of trace plots
- Should look like "fuzzy caterpillars" - random noise around a mean
- All chains should explore the same space
- Patterns, trends, or stuck chains indicate problems

If any of these fail, don't interpret your results! Fix the underlying issue first.
:::

## The Dreaded Divergences üí•

::: {style="background-color: #c62828; padding: 1.5em; border-radius: 0.5em; margin-top: 2em;"}
<div style="color: #ffcdd2; font-size: 1.2em; margin-bottom: 1em;">
  "There were 47 divergences after tuning..."
</div>

<div style="color: #ffcdd2;">
  <div style="font-size: 1.1em; margin-bottom: 1em;">What this means:</div>
  <ul style="font-size: 0.9em;">
    <li>üö® NUTS sampler had numerical problems</li>
    <li>üìä Your results may be biased</li>
    <li>üîç Something is wrong with your model</li>
    <li>‚ö° Don't trust the posterior estimates</li>
  </ul>
</div>
:::

::: {.notes}
Divergences are the most common and concerning diagnostic issue:

What are divergences?
- NUTS uses Hamiltonian dynamics to propose moves
- Divergences occur when the numerical integration becomes unstable
- Usually happens in regions of high curvature or complex geometry
- The sampler "diverges" from the true trajectory

Why they're dangerous:
- Biased sampling: divergences often occur in important regions of the posterior
- Your posterior samples may not represent the true posterior
- Credible intervals may be too narrow
- Point estimates may be wrong

What causes divergences?
- Poorly specified priors (too wide or too narrow)
- Complex model geometry (funnels, multi-modality)
- Highly correlated parameters
- Numerical precision issues
- Model misspecification

The good news: divergences are usually fixable with the right approach. We'll see how in the next slides.
:::

## Diagnosing Sampling Problems

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üö® "Bad Initial Energy"</div>

```python
# First: diagnose the problem
with model:
    model.check_test_point()
    
# Shows which variables have infinite log-prob
# Common fixes:
pm.sample(init='adapt_diag')
pm.sample(init='jitter+adapt_diag')

# Or find reasonable starting point
with model:
    start = pm.find_MAP()  # Use sparingly
    trace = pm.sample(start=start)
```
:::

::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">‚ö° Divergence Solutions</div>

```python
# Step 1: More conservative sampling
pm.sample(target_accept=0.95, tune=2000)

# Step 2: Model reparameterization  
# Non-centered for hierarchical models
theta_raw = pm.Normal('theta_raw', 0, 1)
theta = pm.Deterministic('theta', mu + tau * theta_raw)

# Step 3: Check prior constraints
sigma = pm.HalfNormal('sigma', 1)  # not Normal
```
:::
:::

::: {.notes}
"Bad initial energy" and divergence errors are the top sampling issues beginners face:

"Bad Initial Energy" Diagnosis:
- Cryptic error message but straightforward diagnosis process
- Run model.check_test_point() to identify which variables have infinite log-probability
- Usually caused by inappropriate priors (Normal for positive parameters) 
- Or initialization in impossible regions of parameter space

Practical Fixes:
- init='adapt_diag': better initialization using diagonal mass matrix
- init='jitter+adapt_diag': adds random jitter to starting values  
- pm.find_MAP(): finds mode to start from (use sparingly, discouraged for modern practice)
- Check and fix prior specifications first

Divergence Solutions (Hierarchical):
- "There were X divergences after tuning" is common with hierarchical models
- Knee-jerk reaction: increase target_accept (0.9 to 0.99 often helps)
- Real solution: non-centered parameterization for hierarchical structures
- theta_raw ~ Normal(0,1) then theta = mu + tau * theta_raw
- Separates location (mu) and scale (tau) effects, easier geometry

Systematic Approach:
1. Check model.check_test_point() for infinite values
2. Fix prior constraints (HalfNormal for positive parameters)  
3. Try conservative sampling (target_accept=0.95, tune=2000)
4. Reparameterize hierarchical components if needed
5. Only then consider more advanced techniques

Most "Bad initial energy" errors trace to wrong prior constraints - fix those first.
:::

## Performance Optimization

::: {.columns}
::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üöÄ</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">JAX Backend</div>
  <div style="font-size: 0.9em; color: #888;">5-10x speedup for many models</div>
</div>
:::

::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üìä</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Vectorization</div>
  <div style="font-size: 0.9em; color: #888;">Avoid loops, use tensor operations</div>
</div>
:::

::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üéØ</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Simpler Models</div>
  <div style="font-size: 0.9em; color: #888;">Start simple, add complexity gradually</div>
</div>
:::
:::

```python
# Switch to JAX backend for speed
import pymc as pm
pm.set_backend('jax')

# Or install numpyro for even more speed
# pip install numpyro
```

::: {.notes}
If your models are running slowly, here are the main optimization strategies:

JAX Backend:
- Can provide 5-10x speed improvements
- GPU support for very large models
- JIT compilation optimizes repeated operations
- Install: pip install numpyro
- Switch: pm.set_backend('jax')
- Not all PyMC features supported yet

Vectorization:
- Avoid explicit loops in your model
- Use tensor operations instead
- Let PyMC/NumPy handle the broadcasting
- Example: instead of for loop over observations, use vector operations

Model Simplification:
- Start with the simplest model that could work
- Add complexity only when needed
- More parameters = slower sampling
- Sometimes a simpler model is actually better

Other optimizations:
- Standardize your predictors (zero mean, unit variance)
- Use informative priors when possible
- Consider model approximations (variational inference)
- Profile your code to find bottlenecks

Remember: premature optimization is the root of all evil. Get your model working correctly first, then optimize if needed.
:::

## Prior Specification Problems

::: {.columns}
::: {.column width="50%"}
<div style="background-color: #c62828; padding: 1em; border-radius: 0.5em;">
  <div style="color: #ffcdd2; font-weight: bold; margin-bottom: 0.5em;">‚ùå Wrong Constraints</div>
  <div style="font-size: 0.9em; color: #ffcdd2; font-family: monospace;">
    # Allows negative values!<br>
    sigma = pm.Normal('sigma', 0, 5)
  </div>
  <div style="font-size: 0.9em; color: #ffcdd2; margin-top: 0.5em;">
    ‚Üí "Bad initial energy" errors<br>
    ‚Üí Impossible likelihoods
  </div>
</div>
:::

::: {.column width="50%"}
<div style="background-color: #2e7d32; padding: 1em; border-radius: 0.5em;">
  <div style="color: #c8e6c9; font-weight: bold; margin-bottom: 0.5em;">‚úÖ Proper Constraints</div>
  <div style="font-size: 0.9em; color: #c8e6c9; font-family: monospace;">
    # Ensures positive values<br>
    sigma = pm.HalfNormal('sigma', 5)
  </div>
  <div style="font-size: 0.9em; color: #c8e6c9; margin-top: 0.5em;">
    ‚Üí Stable sampling<br>
    ‚Üí Reasonable constraint
  </div>
</div>
:::
:::

::: {style="background-color: #1565c0; padding: 1em; border-radius: 0.5em; margin-top: 1.5em;"}
<div style="color: #bbdefb; font-weight: bold; margin-bottom: 0.5em;">üéØ Golden Rules</div>
<div style="font-size: 0.9em; color: #bbdefb;">
  ‚Ä¢ <strong>Standard deviations:</strong> HalfNormal, not Normal<br>
  ‚Ä¢ <strong>Probabilities:</strong> Beta(2,2), not Uniform(0,1)<br>
  ‚Ä¢ <strong>Coefficients:</strong> Normal(0, 2.5) for standardized data<br>
  ‚Ä¢ <strong>Always</strong> do prior predictive checks!
</div>
:::

::: {.notes}
The classic beginner mistake: using Normal priors for parameters that must be positive:

Wrong Constraint Priors:
- sigma = pm.Normal('sigma', mu=0, sigma=5) allows negative values
- Creates "Bad initial energy" errors when MCMC draws negative standard deviations  
- Impossible likelihoods when sigma < 0
- This is the most frustrating category of errors for beginners

The Fix is Simple:
- Use constrained distributions: HalfNormal, Exponential, Gamma for positive parameters
- sigma = pm.HalfNormal('sigma', sigma=5) ensures sigma > 0
- Critical for standard deviations, variances, rates, scales

Other Common Prior Mistakes:
- Overly vague priors (sigma=10000) allow unrealistic parameter values
- Uniform priors create sampling difficulties (flat regions are hard to explore)
- Normal priors for probabilities (should use Beta)

Practical Rules for Common Parameters:
- Standard deviations: HalfNormal(sigma=1) or Exponential(lam=1)
- Probabilities: Beta(2, 2) (mild preference for 0.5)
- Correlation coefficients: LKJ prior for positive definite matrices
- Regression coefficients: Normal(0, 2.5) for standardized predictors

The pattern: choose distributions whose support matches parameter constraints. Prior predictive checks catch violations before sampling.
:::

## Debugging Workflow

<div style="font-size: 1.5em; margin-bottom: 2em; text-align: center;">When things go wrong...</div>

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.2em; margin-bottom: 1em;">üîç Diagnostic Steps</div>
<ol style="font-size: 0.9em;">
  <li>Check trace plots</li>
  <li>Look at R-hat values</li>
  <li>Check effective sample size</li>
  <li>Count divergences</li>
  <li>Do posterior predictive checks</li>
</ol>
:::

::: {.column width="50%"}
<div style="font-size: 1.2em; margin-bottom: 1em;">üõ†Ô∏è Fix Strategy</div>
<ol style="font-size: 0.9em;">
  <li>Try sampler tuning first</li>
  <li>Examine prior predictive samples</li>
  <li>Simplify the model</li>
  <li>Reparameterize if needed</li>
  <li>Ask for help on discourse!</li>
</ol>
:::
:::

::: {.notes}
When your model isn't working, follow this systematic debugging approach:

Diagnostic Phase:
1. Trace plots: Do they look like fuzzy caterpillars?
2. R-hat: Are all values < 1.01?
3. ESS: Do you have > 400 effective samples?
4. Divergences: Are there any? Even a few is concerning
5. Posterior predictive: Does the model capture your data?

Fix Strategy:
1. Sampler tuning: Try target_accept=0.95, more tuning steps
2. Prior predictive: Do your priors make sense? Generate reasonable data?
3. Simplify: Remove complexity until it works, then add back gradually
4. Reparameterize: Non-centered parameterization, parameter transforms
5. Get help: PyMC community is very supportive of beginners

Common Progression:
- Start with simplest possible model
- Check it works (good diagnostics)
- Add one piece of complexity at a time
- Diagnose issues immediately when they arise
- Don't move forward with bad diagnostics

Remember: a simple model that works is better than a complex model that doesn't. You can always add complexity later once you have a solid foundation.
:::

## ü§î Conceptual Clarifications

::: {.columns}
::: {.column width="50%"}
<div style="background-color: #ff8f00; padding: 1em; border-radius: 0.5em;">
  <div style="color: #fff3e0; font-weight: bold; margin-bottom: 0.5em;">üó∫Ô∏è find_MAP() Deprecation</div>
  <div style="font-size: 0.9em; color: #fff3e0;">
    <strong>Old tutorials say:</strong> Start with MAP<br>
    <strong>Modern practice:</strong> Just call pm.sample()<br>
    <strong>Why?</strong> MAP isn't representative in high dimensions
  </div>
</div>
:::

::: {.column width="50%"}
<div style="background-color: #1565c0; padding: 1em; border-radius: 0.5em;">
  <div style="color: #bbdefb; font-weight: bold; margin-bottom: 0.5em;">üî¢ PyMC3 vs PyMC</div>
  <div style="font-size: 0.9em; color: #bbdefb;">
    <strong>PyMC3:</strong> Old name (legacy)<br>
    <strong>PyMC:</strong> Current name (v4+)<br>
    <strong>Migration:</strong> Most code works with minor changes
  </div>
</div>
:::
:::

::: {style="background-color: #2e7d32; padding: 1em; border-radius: 0.5em; margin-top: 1.5em;"}
<div style="color: #c8e6c9; font-weight: bold; margin-bottom: 0.5em;">üìä Prior vs Posterior Predictive</div>
<div style="font-size: 0.9em; color: #c8e6c9;">
  <strong>Prior predictive:</strong> Validate model assumptions <em>before</em> seeing data<br>
  <strong>Posterior predictive:</strong> Compare model predictions <em>after</em> fitting<br>
  <strong>Key:</strong> Use joint distributions, not marginals (parameters are correlated!)
</div>
:::

::: {.notes}
Conceptual confusions that persist even after technical issues are resolved:

find_MAP() Deprecation Confusion:
- Older tutorials and books recommend starting with MAP estimates
- find_MAP() was deprecated because MAP estimates aren't representative in high dimensions
- Modern practice: just call pm.sample() directly with good initialization strategies
- NUTS is robust enough to find good starting points automatically
- MAP can actually hinder NUTS sampling by starting in atypical regions

Version Confusion (PyMC3 vs PyMC):
- PyMC3 was renamed to PyMC starting with version 4
- Current PyMC (v5+) is the actively developed version
- Most PyMC3 code works with minor modifications
- Don't panic if you see "PyMC3" in old tutorials - concepts translate directly
- Check import statements: import pymc3 -> import pymc as pm

Prior vs Posterior Predictive Confusion:
- Prior predictive: sample from model before seeing data, validates assumptions
- Posterior predictive: sample from fitted model, compares to observed data  
- Both require joint distributions because parameters are typically correlated
- Common mistake: using marginal distributions instead of joint samples
- Prior predictive catches model specification errors early
- Posterior predictive validates model fit to actual data

These conceptual issues cause anxiety but are easily resolved with modern workflows.
:::

# üåü PyMC Ecosystem

::: {.center}
<div style="font-size: 1.8em; margin-top: 0.5em;">
  Beyond core PyMC: tools that extend your capabilities
</div>
:::

## ArviZ: Your Visualization Partner

::: {.columns}
::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üìä</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Trace Plots</div>
  <div style="font-size: 0.9em;">Check convergence</div>
</div>
:::

::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üé®</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Forest Plots</div>
  <div style="font-size: 0.9em;">Compare parameters</div>
</div>
:::

::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üîç</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Model Checking</div>
  <div style="font-size: 0.9em;">Validate assumptions</div>
</div>
:::
:::

![](/public/plots/bioassay/forest_plot.png){fig-align="center"}

::: {.notes}
ArviZ is the standard tool for Bayesian visualization and diagnostics:

Key Plot Types:

Trace Plots:
- Monitor MCMC convergence
- Identify mixing problems
- Essential for any Bayesian analysis

Forest Plots:
- Compare parameter estimates across models
- Show credible intervals clearly
- Great for hierarchical models with many parameters

Posterior Plots:
- Visualize parameter distributions
- Show credible intervals and point estimates
- Easy to interpret and share

Model Checking:
- Posterior predictive checks
- LOO-CV for model comparison
- Energy plots for NUTS diagnostics
- Rank plots for convergence assessment

Why ArviZ?
- Publication-quality plots out of the box
- Consistent API across different samplers (PyMC, Stan, etc.)
- Extensive customization options
- Integrates seamlessly with PyMC
- Active development and great documentation

The forest plot shown illustrates parameter estimates with uncertainty - much more informative than traditional point estimates + error bars.
:::

## Bambi: High-Level Modeling

```{.python code-line-numbers="1-2|4-5|7-8"}
import bambi as bmb
import pandas as pd

# R-style formula interface
model = bmb.Model('y ~ x1 + x2 + (1|group)', data=df)

# Automatic priors and sampling
results = model.fit()
```

::: {.center}
<div style="font-size: 1.5em; margin-top: 1em;">
  üéØ Like rstanarm/brms for Python!
</div>
:::

::: {.notes}
Bambi provides a high-level interface for common statistical models:

Key Features:

Formula Interface:
- R-style formula syntax: 'y ~ x1 + x2 + (1|group)'
- Automatic handling of categorical variables
- Built-in interactions and transformations
- Hierarchical/mixed effects models

Automatic Priors:
- Sensible default priors based on data
- Weakly informative, properly scaled
- Can override when needed
- Handles centering and scaling automatically

Common Model Types:
- Linear regression
- Logistic regression
- Hierarchical/mixed effects models
- Generalized linear models
- Time series models

When to Use Bambi:
- Standard statistical models
- Quick exploratory analysis
- When you want to focus on interpretation, not model specification
- Teaching statistics (great for students coming from R)

When to Use PyMC Directly:
- Custom models not covered by Bambi
- Need full control over priors and likelihood
- Complex, domain-specific models
- Research applications

Bambi is perfect for 80% of applied statistical modeling, while PyMC handles the remaining 20% of specialized applications.
:::

## PyMC-Experimental: Cutting Edge

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üß™ New Methods</div>
<ul style="font-size: 1.1em;">
  <li>Variational inference</li>
  <li>Gaussian processes</li>
  <li>State space models</li>
  <li>Causal inference</li>
</ul>
:::

::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üöÄ Experimental Features</div>
<ul style="font-size: 1.1em;">
  <li>New samplers</li>
  <li>Advanced diagnostics</li>
  <li>Model comparison</li>
  <li>Optimization tools</li>
</ul>
:::
:::

::: {.center}
<div style="font-size: 1.1em; color: #888; margin-top: 2em;">
  üí° Features graduate to main PyMC when stable
</div>
:::

::: {.notes}
PyMC-experimental is where new features are developed and tested:

New Statistical Methods:
- Variational inference: faster approximate inference
- Gaussian processes: flexible non-parametric models
- State space models: time series with latent states
- Causal inference: experimental designs and effect estimation

Experimental Features:
- New MCMC samplers (beyond NUTS)
- Advanced diagnostics and model checking
- Information criteria and model comparison tools
- Optimization algorithms for MAP estimation

How It Works:
- Install: pip install pymc-experimental
- Import specific modules you need
- Features are less stable than main PyMC
- Extensive testing before moving to core PyMC
- Breaking changes possible between versions

Benefits:
- Access to cutting-edge methods
- Contribute to development through testing
- Preview future PyMC features
- Research applications requiring newest methods

Caution:
- Less documentation than main PyMC
- APIs may change
- Use main PyMC for production applications
- Good for research and experimentation

This is where the future of PyMC is being built!
:::

## Community & Learning Resources

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üìö Learning</div>
<ul style="font-size: 1.1em;">
  <li>üìñ <strong>pymc.io/learn</strong></li>
  <li>üé• YouTube tutorials</li>
  <li>üìì Example gallery</li>
  <li>üìö "Bayesian Analysis with Python"</li>
</ul>
:::

::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em;">üí¨ Community</div>
<ul style="font-size: 1.1em;">
  <li>üó£Ô∏è <strong>discourse.pymc.io</strong></li>
  <li>üê¶ @pymc_devs</li>
  <li>üíª GitHub discussions</li>
  <li>üé™ PyData conferences</li>
</ul>
:::
:::

::: {.center}
<div style="font-size: 1.5em; color: #4FC3F7; margin-top: 2em;">
  ü§ù Welcoming community for all skill levels!
</div>
:::

::: {.notes}
The PyMC community is one of its greatest strengths:

Learning Resources:

Official Documentation:
- pymc.io/learn: comprehensive tutorials and guides
- API reference with examples
- Getting started guides for different backgrounds

Video Content:
- PyMC YouTube channel with tutorials
- Conference talks from PyData events
- Live coding sessions and workshops

Example Gallery:
- 100+ worked examples covering many domains
- Real datasets and complete workflows
- Best practices demonstrated
- Code you can copy and modify

Books:
- "Bayesian Analysis with Python" by Osvaldo Martin
- "Bayesian Methods for Machine Learning" by various authors
- Academic textbooks using PyMC

Community Support:

Discourse Forum (discourse.pymc.io):
- Primary place for questions and discussion
- Searchable archive of solved problems
- Core developers actively participate
- Beginner-friendly atmosphere

Social Media:
- @pymc_devs on Twitter for updates
- LinkedIn PyMC community
- Reddit r/BayesianStatistics

The community motto: "No question is too basic!" Everyone was a beginner once.
:::

## Getting Help & Contributing

::: {.columns}
::: {.column width="50%"}
<div style="background-color: #1565c0; padding: 1.5em; border-radius: 0.5em;">
  <div style="color: #bbdefb; font-size: 1.2em; margin-bottom: 1em;">üÜò Need Help?</div>
  <ul style="color: #bbdefb; font-size: 0.9em;">
    <li>‚úÖ Search discourse.pymc.io first</li>
    <li>‚úÖ Include minimal working example</li>
    <li>‚úÖ Share error messages and diagnostics</li>
    <li>‚úÖ Describe what you've tried</li>
  </ul>
</div>
:::

::: {.column width="50%"}
<div style="background-color: #2e7d32; padding: 1.5em; border-radius: 0.5em;">
  <div style="color: #c8e6c9; font-size: 1.2em; margin-bottom: 1em;">ü§ù Want to Contribute?</div>
  <ul style="color: #c8e6c9; font-size: 0.9em;">
    <li>üìù Documentation improvements</li>
    <li>üêõ Bug reports with examples</li>
    <li>üí° Feature suggestions</li>
    <li>üß™ Testing experimental features</li>
  </ul>
</div>
:::
:::

::: {.notes}
How to get help effectively:

Before Asking:
- Search discourse.pymc.io - your question may already be answered
- Check the documentation and examples
- Try to create a minimal working example

When Asking for Help:
- Include a complete, runnable code example
- Share the full error message (not just "it doesn't work")
- Include your PyMC and Python versions
- Describe what you expected vs what happened
- Show diagnostic plots if relevant

Good Question Example:
```python
import pymc as pm
# minimal data and model code
# error message
# "I expected X but got Y"
```

Ways to Contribute:

Documentation:
- Fix typos and unclear explanations
- Add examples for under-documented features
- Improve tutorials for beginners
- Translate documentation

Bug Reports:
- Include minimal reproducing examples
- Check if already reported on GitHub
- Help developers understand the issue

Feature Requests:
- Describe the use case clearly
- Check if similar functionality exists
- Be willing to help test implementations

Testing:
- Try experimental features and report issues
- Validate examples on different platforms
- Help with pre-release testing

Every contribution, no matter how small, is valued by the community!
:::

# üöÄ Future Directions

::: {.center}
<div style="font-size: 1.8em; margin-top: 0.5em;">
  Where PyMC is heading
</div>
:::

::: {.notes}
Let's look at where PyMC and Bayesian modeling are heading in the next few years.

The field is evolving rapidly, with new developments in:
- Integration with AI/ML workflows
- Computational performance improvements
- New statistical methods and applications
- Easier interfaces for domain experts

Understanding these trends will help you stay current and make good decisions about when and how to adopt new tools.
:::

## ü§ñ AI Integration

::: {.columns}
::: {.column width="50%"}
<div style="text-align: center;">
  <div style="font-size: 4em; margin-bottom: 1em;">üß†</div>
  <div style="font-size: 1.5em;">LLM-Assisted Modeling</div>
  <div style="font-size: 1.1em; margin-top: 0.5em; color: #888;">Natural language ‚Üí PyMC code</div>
</div>
:::

::: {.column width="50%"}
<div style="text-align: center;">
  <div style="font-size: 4em; margin-bottom: 1em;">‚ö°</div>
  <div style="font-size: 1.5em;">Neural Approximations</div>
  <div style="font-size: 1.1em; margin-top: 0.5em; color: #888;">Faster inference with neural networks</div>
</div>
:::
:::

::: {.center}
<div style="font-size: 1.1em; color: #888; margin-top: 2em;">
  "Build a hierarchical model for customer churn prediction"
</div>
:::

::: {.notes}
AI is transforming how we build and use Bayesian models:

LLM-Assisted Modeling:
- Natural language descriptions ‚Üí PyMC code
- Automatic model specification from data descriptions
- Interactive debugging and model improvement
- Democratizing Bayesian modeling for domain experts

Example workflow:
User: "I have sales data by region and want to account for seasonal effects"
AI: Generates hierarchical model with seasonal components in PyMC

Neural Approximations:
- Neural networks to approximate posterior distributions
- Much faster than MCMC for some problems
- Normalizing flows for complex posterior shapes
- Variational autoencoders for model comparison

Current Reality:
- Early stage but rapidly developing
- Some tools already available (GitHub Copilot helps with PyMC code)
- Integration with ChatGPT/Claude for model consultation
- Research active in neural posterior estimation

Future Vision:
- Conversational model building
- Automatic model criticism and improvement
- AI-assisted prior elicitation
- Seamless integration of domain knowledge

This could make Bayesian modeling accessible to many more scientists and analysts who understand their domain but not the technical details of MCMC.
:::

## ‚ö° Performance Revolution

::: {.columns}
::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üöÄ</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">JAX Backend</div>
  <div style="font-size: 0.9em; color: #888;">GPU acceleration</div>
</div>
:::

::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üî•</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Compilation</div>
  <div style="font-size: 0.9em; color: #888;">JIT optimization</div>
</div>
:::

::: {.column width="33%"}
<div style="text-align: center;">
  <div style="font-size: 3em; margin-bottom: 0.5em;">üìà</div>
  <div style="font-size: 1.2em; margin-bottom: 0.5em;">Scalability</div>
  <div style="font-size: 0.9em; color: #888;">Larger datasets</div>
</div>
:::
:::

::: {.center}
<div style="font-size: 1.5em; margin-top: 2em;">
  100x speedups possible for large models
</div>
:::

::: {.notes}
Performance improvements are making Bayesian modeling feasible for much larger problems:

JAX Backend:
- GPU/TPU acceleration for massive models
- Automatic differentiation optimized for modern hardware
- Vectorized operations across multiple devices
- Already 5-10x faster for many models

JIT Compilation:
- XLA compiler optimizes entire model graphs
- Eliminates Python overhead
- Aggressive optimization for repeated operations
- Cold start penalty but then much faster

Scalability Improvements:
- Handle millions of observations
- Thousands of parameters in hierarchical models
- Real-time inference for streaming data
- Distributed computation across clusters

Current Status:
- JAX backend already available and improving
- Some limitations vs NumPy backend
- Research into specialized hardware (TPUs)
- Integration with cloud computing platforms

Impact:
- Previously impossible models now feasible
- Real-time decision making with Bayesian uncertainty
- Large-scale A/B testing and experimentation
- Industrial applications with big data

This performance revolution is making Bayesian methods competitive with classical ML for large-scale applications.
:::

## üåç Growing Applications

::: {.columns}
::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em; color: #9C27B0;">üî¨ Science</div>
<ul style="font-size: 0.9em;">
  <li>Climate modeling</li>
  <li>Genomics & personalized medicine</li>
  <li>Astronomy & cosmology</li>
  <li>Social sciences</li>
</ul>
:::

::: {.column width="50%"}
<div style="font-size: 1.5em; margin-bottom: 1em; color: #2196F3;">üè¢ Industry</div>
<ul style="font-size: 0.9em;">
  <li>Supply chain optimization</li>
  <li>Financial risk modeling</li>
  <li>Marketing mix modeling</li>
  <li>Quality control</li>
</ul>
:::
:::

::: {.center}
<div style="font-size: 1.1em; color: #888; margin-top: 2em;">
  Uncertainty quantification becoming essential everywhere
</div>
:::

::: {.notes}
Bayesian methods are expanding into new domains:

Scientific Applications:
- Climate modeling: quantifying uncertainty in climate projections
- Genomics: personalized medicine with individual-level predictions
- Astronomy: discovering exoplanets and dark matter
- Social sciences: causal inference and policy evaluation

Industrial Applications:
- Supply chain: robust optimization under uncertainty
- Finance: regulatory compliance requires uncertainty quantification
- Marketing: attribution and media mix optimization
- Manufacturing: quality control and predictive maintenance

Why the Growth?
- Regulatory requirements for uncertainty quantification
- AI safety concerns driving need for calibrated confidence
- Better tools making Bayesian methods more accessible
- Recognition that point estimates are insufficient

Market Trends:
- Consulting firms building Bayesian practices
- Software companies integrating uncertainty quantification
- Academic programs emphasizing Bayesian statistics
- Industry standards requiring probabilistic forecasts

The future is probabilistic - organizations that embrace uncertainty quantification will have competitive advantages in decision-making under uncertainty.
:::

## ü§ù How You Can Contribute

::: {.columns}
::: {.column width="50%"}
<div style="background-color: #1565c0; padding: 1.5em; border-radius: 0.5em;">
  <div style="color: #bbdefb; font-size: 1.2em; margin-bottom: 1em;">üë©‚Äçüíª Technical</div>
  <ul style="color: #bbdefb; font-size: 0.9em;">
    <li>üêõ Report bugs with examples</li>
    <li>üìö Improve documentation</li>
    <li>üß™ Test experimental features</li>
    <li>üí° Contribute code improvements</li>
  </ul>
</div>
:::

::: {.column width="50%"}
<div style="background-color: #2e7d32; padding: 1.5em; border-radius: 0.5em;">
  <div style="color: #c8e6c9; font-size: 1.2em; margin-bottom: 1em;">üåü Community</div>
  <ul style="color: #c8e6c9; font-size: 0.9em;">
    <li>‚ùì Answer questions on discourse</li>
    <li>‚úçÔ∏è Write tutorials and blog posts</li>
    <li>üé§ Give talks at meetups</li>
    <li>üë• Organize local user groups</li>
  </ul>
</div>
:::
:::

::: {.center}
<div style="font-size: 1.5em; color: #FF9800; margin-top: 2em;">
  üöÄ Join us in building the future of data science!
</div>
:::

::: {.notes}
The PyMC community thrives on contributions from users like you:

Technical Contributions:

Bug Reports:
- Include minimal reproducing examples
- Help developers understand and fix issues
- Make PyMC more reliable for everyone

Documentation:
- Fix typos and unclear explanations
- Add examples for complex features
- Improve tutorials for beginners
- Translate content to other languages

Testing:
- Try new features before release
- Report compatibility issues
- Help with cross-platform testing

Code:
- Fix bugs in your area of expertise
- Implement new features
- Optimize performance
- Add new distributions or samplers

Community Contributions:

Support:
- Answer questions on discourse.pymc.io
- Help newcomers get started
- Share your expertise

Content Creation:
- Write blog posts about your analyses
- Create video tutorials
- Share case studies from your domain
- Publish reproducible research

Outreach:
- Give talks at conferences and meetups
- Organize workshops at your institution
- Start local PyMC user groups
- Advocate for Bayesian methods in your field

Every contribution matters - the community values all forms of participation!
:::

# Questions? ü§î

::: {.center}
<div style="font-size: 1.5em; margin-top: 2em; margin-bottom: 3em;">
  Thank you for joining this PyMC journey!
</div>
:::

::: {.columns}
::: {.column width="33%"}
::: {.center}
<div style="font-size: 2em; margin-bottom: 0.5em;">üí¨</div>
<div>discourse.pymc.io</div>
:::
:::

::: {.column width="33%"}
::: {.center}
<div style="font-size: 2em; margin-bottom: 0.5em;">üìö</div>
<div>pymc.io</div>
:::
:::

::: {.column width="33%"}
::: {.center}
<div style="font-size: 2em; margin-bottom: 0.5em;">ü¶ã</div>
<div>@pymc.io</div>
:::
:::
:::

::: {.center}
<div style="color: #888; margin-top: 2em;">
  Slides: github.com/fonnesbeck/du_pymc_tutorial
</div>
:::

::: {.notes}
Thank you for joining this comprehensive introduction to PyMC!

Quick recap of what we covered:
- What PyMC is and why Bayesian modeling matters
- Installation and setup (easier than you might think!)
- Core concepts: models, distributions, observed data
- Complete worked example with real data and diagnostics
- Common pitfalls and how to avoid them
- The broader ecosystem and community resources
- Future directions and opportunities to contribute

Key takeaways:
1. Bayesian modeling gives you uncertainty quantification for free
2. PyMC makes complex statistical modeling accessible
3. Always check your diagnostics before trusting results
4. The community is welcoming and supportive
5. Start simple and add complexity gradually

Next steps:
- Try the bioassay example yourself
- Apply PyMC to your own data
- Join the discourse forum for support
- Explore the example gallery for inspiration

Resources for continued learning:
- discourse.pymc.io: get help and see discussions
- pymc.io: official documentation and tutorials
- @pymc_devs: stay updated on new developments

Remember: every expert was once a beginner. Don't hesitate to ask questions and engage with the community!

Happy Bayesian modeling! üéâ
:::